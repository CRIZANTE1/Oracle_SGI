import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import time
from sklearn.metrics.pairwise import cosine_similarity
import logging
import re # Importar a biblioteca de express√µes regulares

logging.basicConfig(level=logging.INFO)

@st.cache_data(ttl=3600)
def load_preprocessed_rag_base() -> tuple[pd.DataFrame | None, np.ndarray | None]:
    """
    Carrega o DataFrame e os embeddings pr√©-processados de arquivos locais.
    """
    try:
        df = pd.read_pickle("rag_dataframe.pkl")
        embeddings = np.load("gemini_embeddings_001.npy")
        logging.info("Base de conhecimento (RAG) carregada com sucesso do cache.")
        return df, embeddings
    except FileNotFoundError:
        logging.error("Arquivos da base de conhecimento n√£o encontrados.")
        return None, None
    except Exception as e:
        logging.error(f"Falha cr√≠tica ao carregar a base de conhecimento: {e}")
        return None, None

class GeminiRAG:
    def __init__(self, api_key: str):
        """
        Inicializa o sistema RAG, configurando a API do Gemini e carregando a base de dados.
        """
        self.model_generator = None
        self._ready = False

        with st.spinner("Carregando base de conhecimento..."):
            self.rag_df, self.rag_embeddings = load_preprocessed_rag_base()

        if self.rag_df is None or self.rag_embeddings is None:
            st.error("ERRO CR√çTICO: Arquivos da base de conhecimento n√£o encontrados.")
            self.rag_df = pd.DataFrame()
            self.rag_embeddings = np.array([])
            return
        else:
            required_cols = ['Norma', 'Referencia', 'Answer_Chunk']
            if not all(col in self.rag_df.columns for col in required_cols):
                st.error(f"ERRO CR√çTICO: O DataFrame n√£o cont√©m as colunas necess√°rias: {required_cols}.")
                self._ready = False
                return
            st.toast("Base de conhecimento carregada com sucesso.", icon="üß†")

        try:
            if not api_key:
                st.error("A chave da API do Gemini n√£o foi fornecida.")
                raise ValueError("A chave da API n√£o pode ser vazia.")
                
            genai.configure(api_key=api_key)
            self.model_generator = genai.GenerativeModel('gemini-2.5-pro')
            logging.info("Modelo Gerador (Gemini) configurado com sucesso.")
            self._ready = True

        except Exception as e:
            st.error(f"Erro ao inicializar o modelo Gemini: {e}")
            self._ready = False
            raise

    def is_ready(self) -> bool:
        """Verifica se o sistema RAG est√° pronto para uso."""
        return self._ready

    def _find_relevant_chunks(self, query_text: str, top_k: int = 5) -> str:
        """
        Encontra os chunks mais relevantes e retorna o texto junto com sua refer√™ncia normativa.
        """
        if not self.is_ready():
            return "Base de conhecimento indispon√≠vel."

        try:
            result = genai.embed_content(model='models/embedding-001', content=query_text)
            query_embedding = np.array(result['embedding']).reshape(1, -1)
            
            similarities = cosine_similarity(query_embedding, self.rag_embeddings)[0]
            top_k_indices = similarities.argsort()[-top_k:][::-1]
            
            relevant_chunks_df = self.rag_df.iloc[top_k_indices]
            
            formatted_chunks = []
            for index, row in relevant_chunks_df.iterrows():
                reference = f"{row['Norma']} - {row['Referencia']}"
                chunk_text = row['Answer_Chunk']
                formatted_chunks.append(f"[Fonte: {reference}]\n{chunk_text}")

            context = "\n\n---\n\n".join(formatted_chunks)
            return context
        except Exception as e:
            st.error(f"Erro durante a busca sem√¢ntica com o Gemini. Detalhes: {e}")
            return "Erro ao buscar informa√ß√µes relevantes."

    # --- NOVA FUN√á√ÉO AUXILIAR PARA DETECTAR A INTEN√á√ÉO ---
    def _is_reference_query(self, question: str) -> bool:
        """Verifica se a pergunta do usu√°rio √© um pedido de refer√™ncias."""
        question_lower = question.lower()
        # Palavras-chave que indicam um pedido de fontes/refer√™ncias
        keywords = ['refer√™ncia', 'referencias', 'norma', 'normas', 'fonte', 'fontes', 'cl√°usula', 'clausula', 'item', 'itens']
        return any(keyword in question_lower for keyword in keywords)

    # --- FUN√á√ÉO PRINCIPAL ATUALIZADA COM A NOVA L√ìGICA ---
    def answer_question(self, question: str) -> tuple[str, float]:
        """
        Orquestra a resposta. Primeiro, detecta a inten√ß√£o do usu√°rio.
        - Se for um pedido de refer√™ncias, extrai e lista as fontes.
        - Se for uma pergunta de conte√∫do, usa o RAG completo com o Gemini.
        """
        if not self.is_ready():
            return "O sistema de IA n√£o est√° operacional.", 0

        start_time = time.time()
        
        # Passo 1: Busca sem√¢ntica para encontrar os chunks relevantes
        relevant_context = self._find_relevant_chunks(question, top_k=10)
        
        # Passo 2: Verifica√ß√£o de Erro na Busca
        if "Erro" in relevant_context or not relevant_context.strip():
            answer = "N√£o foi poss√≠vel consultar a base de conhecimento ou encontrar informa√ß√µes relevantes."
        
        # Passo 3: Detector de Inten√ß√£o
        elif self._is_reference_query(question):
            # L√≥gica para extrair e listar refer√™ncias
            st.info("Inten√ß√£o detectada: Pedido de refer√™ncias normativas.")
            # Extrai todas as fontes do contexto usando express√£o regular
            references = re.findall(r'\[Fonte: (.*?)\]', relevant_context)
            
            if references:
                # Remove duplicatas mantendo a ordem
                unique_references = sorted(list(set(references)), key=references.index)
                
                # Formata a resposta como uma lista markdown
                answer = "### Refer√™ncias Normativas Encontradas:\n\n"
                answer += "\n".join([f"- {ref}" for ref in unique_references])
            else:
                answer = "Nenhuma refer√™ncia normativa espec√≠fica foi encontrada para os termos da sua busca."
        
        else:
            # L√≥gica RAG normal para responder perguntas de conte√∫do
            prompt = f"""
            **Persona:** Voc√™ √© um Consultor Especialista em Normas Regulamentadoras, cuja maior prioridade √© a precis√£o e a rastreabilidade da informa√ß√£o. Sua comunica√ß√£o √© did√°tica e **sempre referenciada**.
            **Miss√£o Cr√≠tica:** Fornecer uma resposta completa e detalhada √† **Pergunta do Usu√°rio**, baseando-se **√∫nica e exclusivamente** nas informa√ß√µes do **Contexto Relevante**. O aspecto mais importante da sua tarefa √© citar as fontes de cada informa√ß√£o.
            **Formato do Contexto:** Cada trecho de informa√ß√£o no contexto √© precedido por sua fonte no formato `[Fonte: Norma - Refer√™ncia]`.
            **INSTRU√á√ïES DETALHADAS PARA A RESPOSTA:**
            1. **S√≠ntese e Elabora√ß√£o:** Analise todos os trechos do contexto. Sintetize as informa√ß√µes para construir uma resposta coesa e detalhada, explicando os conceitos chave.
            2. **Cita√ß√£o Obrigat√≥ria:** Ao formular sua resposta, voc√™ **DEVE** citar a(s) fonte(s) normativa(s) de onde extraiu a informa√ß√£o. Integre a cita√ß√£o de forma natural no texto. Exemplo: "... o procedimento de inspe√ß√£o deve seguir as recomenda√ß√µes do fabricante, conforme especificado na **NBR 9442 - 8.1.4.1**."
            3. **Estrutura Clara:** Organize a resposta de forma l√≥gica, usando par√°grafos, listas e **negrito** para destacar termos importantes.
            4. **Se√ß√£o de Fontes:** Ao final de **TODA** a sua resposta, adicione uma se√ß√£o chamada "**Fontes Consultadas**" e liste todas as refer√™ncias `[Fonte: ...]` que voc√™ utilizou para construir a resposta.
            5. **Fidelidade Absoluta (REGRA INQUEBR√ÅVEL):** Se o contexto n√£o cont√©m informa√ß√µes para responder √† pergunta, responda apenas: *"Com base estrita no contexto fornecido, n√£o h√° informa√ß√µes sobre o t√≥pico solicitado."*
            ---
            **Contexto Relevante (Sua √∫nica fonte da verdade):**
            {relevant_context}
            ---
            **Pergunta do Usu√°rio:**
            {question}
            **Sua Resposta (Siga TODAS as instru√ß√µes, incluindo a cita√ß√£o no texto e a lista de fontes ao final):**
            """
            
            try:
                response = self.model_generator.generate_content(prompt)
                answer = response.text
            except Exception as e:
                st.error(f"Erro ao gerar a resposta com o modelo Gemini: {e}")
                answer = "Ocorreu um erro ao tentar gerar a resposta final."

        end_time = time.time()
        elapsed_time = end_time - start_time
        return answer, elapsed_time
