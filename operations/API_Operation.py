import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import time
from sklearn.metrics.pairwise import cosine_similarity
import logging

logging.basicConfig(level=logging.INFO)

@st.cache_data(ttl=3600)
def load_preprocessed_rag_base() -> tuple[pd.DataFrame | None, np.ndarray | None]:
    """
    Carrega o DataFrame e os embeddings pr√©-processados de arquivos locais.
    """
    try:
        df = pd.read_pickle("rag_dataframe.pkl")
        # --- MUDAN√áA IMPORTANTE: Carregando o novo arquivo de embeddings ---
        embeddings = np.load("gemini_embeddings_001.npy")
        logging.info("Base de conhecimento (RAG) carregada com sucesso do cache.")
        return df, embeddings
    except FileNotFoundError:
        logging.error("Arquivos da base de conhecimento ('rag_dataframe.pkl' ou 'gemini_embeddings_001.npy') n√£o encontrados.")
        return None, None
    except Exception as e:
        logging.error(f"Falha cr√≠tica ao carregar a base de conhecimento pr√©-processada: {e}")
        return None, None

class GeminiRAG:
    def __init__(self, api_key: str):
        """
        Inicializa o sistema RAG, configurando a API do Gemini e carregando a base de dados.
        """
        self.model_generator = None
        self._ready = False

        with st.spinner("Carregando base de conhecimento..."):
            self.rag_df, self.rag_embeddings = load_preprocessed_rag_base()

        if self.rag_df is None or self.rag_embeddings is None:
            st.error("ERRO CR√çTICO: Arquivos da base de conhecimento ('rag_dataframe.pkl' ou 'gemini_embeddings_001.npy') n√£o encontrados. A funcionalidade de IA ser√° desativada.")
            self.rag_df = pd.DataFrame()
            self.rag_embeddings = np.array([])
            return
        else:
            st.toast("Base de conhecimento carregada com sucesso.", icon="üß†")

        try:
            if not api_key:
                st.error("A chave da API do Gemini n√£o foi fornecida.")
                raise ValueError("A chave da API n√£o pode ser vazia.")
                
            genai.configure(api_key=api_key)
            self.model_generator = genai.GenerativeModel('gemini-2.5-pro')
            logging.info("Modelo Gerador (Gemini) configurado com sucesso.")
            self._ready = True

        except Exception as e:
            st.error(f"Erro ao inicializar o modelo Gemini. Verifique se a chave da API √© v√°lida. Detalhes: {e}")
            logging.error(f"Erro durante a inicializa√ß√£o do modelo Gemini: {e}")
            self._ready = False
            raise

    def is_ready(self) -> bool:
        """Verifica se o sistema RAG est√° pronto para uso."""
        return self._ready

    def _find_relevant_chunks(self, query_text: str, top_k: int = 5) -> str:
        """
        Encontra os chunks mais relevantes usando Gemini/embedding-001.
        """
        if not self.is_ready():
            return "Base de conhecimento indispon√≠vel."

        try:
            result = genai.embed_content(
                model='models/embedding-001',
                content=query_text
            )
            query_embedding = np.array(result['embedding']).reshape(1, -1)
            # --- FIM DA MUDAN√áA ---
            
            similarities = cosine_similarity(query_embedding, self.rag_embeddings)[0]
            top_k_indices = similarities.argsort()[-top_k:][::-1]
            relevant_chunks = self.rag_df.iloc[top_k_indices]
            
            context = "\n\n---\n\n".join(relevant_chunks['Answer_Chunk'].tolist())
            return context
        except Exception as e:
            st.error(f"Erro durante a busca sem√¢ntica com o Gemini. Detalhes: {e}")
            return "Erro ao buscar informa√ß√µes relevantes."

    def answer_question(self, question: str) -> tuple[str, float]:
        """
        Orquestra o processo: busca e gera√ß√£o, ambos com Gemini.
        """
        if not self.is_ready():
            return "O sistema de IA n√£o est√° operacional.", 0

        start_time = time.time()
        relevant_context = self._find_relevant_chunks(question, top_k=7)
        
        if "Erro" in relevant_context or not relevant_context.strip():
            answer = "N√£o foi poss√≠vel consultar a base de conhecimento ou encontrar informa√ß√µes relevantes para responder √† sua pergunta."
        else:
            prompt = f"""
            **Persona:** Voc√™ √© um Consultor Especialista em Normas Regulamentadoras e Sa√∫de e Seguran√ßa do Trabalho. Sua comunica√ß√£o √© did√°tica, clara e completa.

            **Miss√£o Cr√≠tica:** Sua miss√£o √© fornecer uma resposta completa, detalhada e bem estruturada √† **Pergunta do Usu√°rio**, baseando-se **√∫nica e exclusivamente** nas informa√ß√µes contidas no **Contexto Relevante**.

            **INSTRU√á√ïES DETALHADAS PARA A RESPOSTA:**

            1.  **S√≠ntese Abrangente:** Analise **TODOS** os trechos do contexto fornecido. Se m√∫ltiplos trechos abordam o mesmo t√≥pico, sintetize as informa√ß√µes para construir uma resposta coesa e abrangente, conectando as ideias.

            2.  **Elabora√ß√£o e Detalhamento:** N√£o se limite a uma resposta curta. Elabore sobre os pontos encontrados, explique os conceitos chave, detalhe os processos ou requisitos mencionados e, se o contexto permitir, cite exemplos ou condi√ß√µes espec√≠ficas. O objetivo √© educar o usu√°rio sobre o tema.

            3.  **Estrutura e Clareza:** Organize sua resposta de forma l√≥gica. Utilize par√°grafos para separar ideias e, quando apropriado, use listas (bullet points) para apresentar itens, etapas ou requisitos de forma clara e f√°cil de ler. Use **negrito** para destacar os termos t√©cnicos ou os pontos mais importantes.

            4.  **Fidelidade Absoluta (REGRA INQUEBR√ÅVEL):** Se o contexto fornecido n√£o cont√©m informa√ß√µes suficientes para responder √† pergunta do usu√°rio, sua √∫nica a√ß√£o √© responder com a seguinte frase: *"Com base estrita no contexto fornecido, n√£o h√° informa√ß√µes detalhadas sobre o t√≥pico solicitado."* **N√ÉO** invente informa√ß√µes ou use conhecimento externo.

            ---
            **Contexto Relevante (Sua √∫nica fonte da verdade):**
            {relevant_context}
            ---

            **Pergunta do Usu√°rio:**
            {question}

            **Sua Resposta (Siga as instru√ß√µes para uma resposta detalhada e estruturada):**
            """
            
            try:
                response = self.model_generator.generate_content(prompt)
                answer = response.text
            except Exception as e:
                st.error(f"Erro ao gerar a resposta com o modelo Gemini: {e}")
                answer = "Ocorreu um erro ao tentar gerar a resposta final."

        end_time = time.time()
        elapsed_time = end_time - start_time
        return answer, elapsed_time
