import streamlit as st
import google.generativeai as genai
import pandas as pd
import numpy as np
import time
from sklearn.metrics.pairwise import cosine_similarity
import logging

logging.basicConfig(level=logging.INFO)

@st.cache_data(ttl=3600)
def load_preprocessed_rag_base() -> tuple[pd.DataFrame | None, np.ndarray | None]:
    """
    Carrega o DataFrame e os embeddings prﾃｩ-processados de arquivos locais.
    Retorna None em caso de falha para que a classe possa lidar com o erro.
    """
    try:
        df = pd.read_pickle("rag_dataframe.pkl")
        embeddings = np.load("rag_embeddings.npy")
        logging.info("Base de conhecimento (RAG) carregada com sucesso do cache.")
        return df, embeddings
    except FileNotFoundError:
        logging.error("Arquivos da base de conhecimento ('rag_dataframe.pkl' ou 'rag_embeddings.npy') nﾃ｣o encontrados.")
        return None, None
    except Exception as e:
        logging.error(f"Falha crﾃｭtica ao carregar a base de conhecimento prﾃｩ-processada: {e}")
        return None, None

class GeminiRAG:
    def __init__(self, api_key: str):
        """
        Inicializa o sistema RAG, configurando a API do Gemini e carregando a base de dados.
        """
        self.model = None
        self._ready = False

        if not api_key:
            st.error("A chave da API fornecida estﾃ｡ vazia.")
            raise ValueError("A chave da API nﾃ｣o pode ser vazia.")
        
        with st.spinner("Carregando base de conhecimento..."):
            self.rag_df, self.rag_embeddings = load_preprocessed_rag_base()

        if self.rag_df is None or self.rag_embeddings is None:
            st.error("ERRO CRﾃ控ICO: Arquivos da base de conhecimento ('rag_dataframe.pkl' ou 'rag_embeddings.npy') nﾃ｣o encontrados. A funcionalidade de IA serﾃ｡ desativada.")
            self.rag_df = pd.DataFrame()
            self.rag_embeddings = np.array([])
            return
        else:
            st.toast("Base de conhecimento carregada com sucesso.", icon="洫")

        try:
            genai.configure(api_key=api_key)
            self.model = genai.GenerativeModel('gemini-1.5-pro-latest')
            logging.info("Modelo Gemini configurado com sucesso.")
            self._ready = True

        except Exception as e:
            st.error(f"Erro ao inicializar o modelo Gemini. Verifique se a chave da API ﾃｩ vﾃ｡lida. Detalhes: {e}")
            logging.error(f"Erro durante a inicializaﾃｧﾃ｣o da classe GeminiRAG: {e}")
            self._ready = False
            raise

    def is_ready(self) -> bool:
        """Verifica se o sistema RAG estﾃ｡ pronto para uso."""
        return self._ready

    def _find_relevant_chunks(self, query_text: str, top_k: int = 5) -> str:
        """
        Encontra os chunks mais relevantes na base de conhecimento para uma dada pergunta.
        """
        if not self.is_ready():
            return "Base de conhecimento indisponﾃｭvel."

        try:
            query_embedding_result = genai.embed_content(
                model='models/text-embedding-004',
                content=[query_text],
                task_type="RETRIEVAL_QUERY"
            )
            query_embedding = np.array(query_embedding_result['embedding']).reshape(1, -1)
            
            similarities = cosine_similarity(query_embedding, self.rag_embeddings)[0]
            top_k_indices = similarities.argsort()[-top_k:][::-1]
            relevant_chunks = self.rag_df.iloc[top_k_indices]
            
            context = "\n\n---\n\n".join(relevant_chunks['Answer_Chunk'].tolist())
            return context
        except Exception as e:
            st.warning(f"Erro durante a busca na base de conhecimento: {e}")
            return "Erro ao buscar informaﾃｧﾃｵes relevantes."

    def answer_question(self, question: str) -> tuple[str, float]:
        """
        Orquestra o processo de responder a uma pergunta usando RAG.
        """
        if not self.is_ready():
            return "O sistema de IA nﾃ｣o estﾃ｡ operacional.", 0

        start_time = time.time()
        
        relevant_context = self._find_relevant_chunks(question, top_k=7)
        
        if "indisponﾃｭvel" in relevant_context or "Erro" in relevant_context:
            answer = "Nﾃ｣o foi possﾃｭvel consultar a base de conhecimento para responder ﾃ sua pergunta."
        else:
            # --- PROMPT APRIMORADO ---
            prompt = f"""
            **Persona:** Vocﾃｪ ﾃｩ um Orﾃ｡culo Analﾃｭtico, especialista na norma ISO 45001.

            **Missﾃ｣o Crﾃｭtica:** Sua tarefa ﾃｩ responder ﾃ **Pergunta do Usuﾃ｡rio** usando **ﾃｺnica e exclusivamente** as informaﾃｧﾃｵes contidas no **Contexto Relevante** fornecido abaixo. Sua fidelidade ao texto ﾃｩ absoluta.

            **REGRAS DE OURO (Nﾃグ QUEBRE ESTAS REGRAS):**

            1.  **SE A RESPOSTA ESTIVER NO CONTEXTO:** Responda ﾃ pergunta de forma clara e objetiva, baseando-se estritamente nos trechos fornecidos. Vocﾃｪ pode citar ou parafrasear o conteﾃｺdo, mas nﾃ｣o adicione informaﾃｧﾃｵes externas.

            2.  **SE A RESPOSTA Nﾃグ ESTIVER NO CONTEXTO:** Esta ﾃｩ a regra mais importante. Se o contexto nﾃ｣o contﾃｩm informaﾃｧﾃｵes sobre o tema da pergunta, sua ﾃｺnica aﾃｧﾃ｣o ﾃｩ responder com uma declaraﾃｧﾃ｣o clara de que a informaﾃｧﾃ｣o nﾃ｣o foi encontrada.
                - **Nﾃグ** tente adivinhar a resposta.
                - **Nﾃグ** utilize seu conhecimento geral sobre outros assuntos ou normas (como NR-01, PGR, NR-35, etc.).
                - **Nﾃグ** resuma o conteﾃｺdo do contexto se ele for irrelevante para a pergunta. Simplesmente declare que o tﾃｳpico especﾃｭfico nﾃ｣o foi abordado.

            **Exemplo de uma recusa correta:**
            Se a pergunta for "O que ﾃｩ o PGR da NR-01?" e o contexto sﾃｳ falar de ISO 45001, sua resposta deve ser:
            *"Com base estrita no contexto fornecido, nﾃ｣o hﾃ｡ informaﾃｧﾃｵes sobre o PGR (Programa de Gerenciamento de Riscos) ou a NR 01."*

            ---
            **Contexto Relevante (Sua ﾃｺnica fonte da verdade):**
            {relevant_context}
            ---

            **Pergunta do Usuﾃ｡rio:**
            {question}

            **Sua Resposta (Siga as Regras de Ouro):**
            """
            
            try:
                response = self.model.generate_content(prompt)
                answer = response.text
            except Exception as e:
                st.error(f"Erro ao gerar a resposta com o modelo de IA: {e}")
                answer = "Ocorreu um erro ao tentar gerar a resposta final."

        end_time = time.time()
        elapsed_time = end_time - start_time

        return answer, elapsed_time

